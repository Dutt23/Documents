awesome — let’s tune end-to-end for huge S3 uploads (think: multi-GB/ TB files). I’ll go stage-by-stage along the path you already grok (app → TCP/IP → qdisc → NIC) and add the S3-specific moves (multipart, concurrency, network path choices). You can cherry-pick per your setup (on-prem vs EC2).

1) App layer (read → encrypt → send)

Goal: feed the kernel in big, steady chunks with minimal copies while keeping enough in-flight to fill the pipe.

Multipart Upload (mandatory for big files):

Use multipart with parallel parts. Target 8–128 MiB per part for big objects.

Keep ≤10,000 parts ⇒ part_size ≥ ceil(object_size / 10000).

Concurrency rule of thumb:

Compute BDP = bandwidth (bytes/s) × RTT (s); ensure total in-flight ≥ BDP.

Start with 8–32 parallel parts, adjust until throughput plateaus.



Connection reuse: one HTTP(S) connection per part (keep-alive). Don’t re-handshake per part.

TLS cost: prefer AES-GCM with AES-NI or ChaCha20-Poly1305 (CPU dependent). Keep a single crypto context per connection; pin upload threads to CPUs with crypto extensions.

I/O path from disk:

Use readv/large buffered reads or io_uring async reads to keep the pipeline full.

Pre-fetch / increase readahead for sequential files (blockdev --setra or posix_fadvise(SEQUENTIAL)).


Zero-copy possibilities:

If you control the stack in C and your kernel/OpenSSL support kTLS, you can do file→socket sendfile() with TX kTLS to avoid user-space copies. (Most SDKs don’t expose this; it’s an advanced optimization.)



SDK knobs (examples)

Java (AWS SDK v2 + CRT/Netty):

maxConcurrency (multipart parallelism) 8–64, targetThroughputInGbps if using Transfer Manager.

Increase connection pool (SdkAsyncHttpClient.Builder.maxConcurrency()).

HTTP/1.1 keep-alive enabled; tune writeTimeout, tcpKeepAlive.


Python (boto3 / s3transfer):

TransferConfig(multipart_threshold=8*1024**2, multipart_chunksize=64*1024**2, max_concurrency=32, use_threads=True)



2) Transport (TCP) — batching & pacing

Goal: fewer packets per byte, stable pacing, enough window to match BDP.

Enable/keep: TSO/GSO, checksum offload (NIC).

Congestion control: BBR for high-BDP/lossy long-haul; CUBIC is fine intra-DC.
sysctl net.ipv4.tcp_congestion_control=bbr

Buffers (ceilings; Linux autotunes within these):

sysctl net.core.wmem_max=268435456
sysctl net.ipv4.tcp_wmem="4096 87380 268435456"

Prevent giant unsent queues (lower tail latency & memory):

TCP_NOTSENT_LOWAT (per-socket) to ~1–8 MiB when parts are many.


Coalescing vs latency:

For big streaming PUTs, autocorking/TCP_CORK helps make large GSO segments.

For many tiny metadata calls, use TCP_NODELAY.



3) Network (IP, policy, path choice)

Goal: shortest/cleanest path to S3 with stable RTT and MTU.

From EC2 to S3 (same region):

Use S3 VPC Endpoint; traffic stays on AWS backbone.

ENA enhanced networking on instance; MTU 9001 inside VPC path where possible.


From on-prem to S3:

Choose the closest S3 region (test RTT).

Consider S3 Transfer Acceleration (edge → AWS backbone) if RTT/packet loss to region is high.

Ensure a clean path: correct MSS/PMTU, avoid middleboxes that kill large TCP options/ECN.


ECN & DSCP: if your network honors ECN, enable it; avoid marking that confuses WAN providers.


4) qdisc (software TX queue)

Goal: smooth bursts and avoid bufferbloat as multiple parts run.

Use sch_fq for per-flow pacing (great with BBR and many parallel PUTs):
tc qdisc replace dev eth0 root fq

Keep txqueuelen moderate (e.g., 2000–5000 on fast links) to avoid large standing queues.


5) NIC / CPU placement (egress steering)

Goal: avoid cross-CPU/NUMA bouncing as multiple flows transmit.

XPS (Transmit Packet Steering): map TX queues to CPUs doing the sending:

echo 2-7  > /sys/class/net/eth0/queues/tx-0/xps_cpus
echo 8-13 > /sys/class/net/eth0/queues/tx-1/xps_cpus

NUMA pinning: run uploader threads and allocate buffers on the NIC’s NUMA node:
numactl --cpunodebind=<node> --membind=<node> <uploader>

Rings & coalescing:

ethtool -G eth0 tx 4096
ethtool -C eth0 tx-usecs 64
ethtool -K eth0 tso on gso on tx on

Jumbo MTU (in VPC/datacenter only if end-to-end clean): ip link set dev eth0 mtu 9000


6) Disk → CPU → NIC pipeline (don’t let one stage starve the others)

Goal: steady state, no bubbles.

Match read chunk size to multipart chunk (e.g., 64 MiB reads feeding 64 MiB parts).

Keep N parts in progress such that N × part_size / avg_part_time ≈ link_rate.

If CPU-bound on TLS, scale upload workers = number of physical cores with AES-NI (or pin threads).

Consider compress-then-upload only if the content compresses a lot; otherwise you waste CPU and slow the pipe.


7) Observability & feedback loop

Measure each layer so you know which knob to turn:

App/SDK: part durations, bytes/s per part, concurrency, HTTP 5xx/slowdowns.

TCP/socket: ss -ti (cwnd, pacing rate, notsent, rtt).

qdisc: tc -s qdisc show dev eth0 (backlog/drops).

NIC: ethtool -S eth0 (per-queue TX stats, tx_dropped).

CPU profiling: look for hotspots in TLS, copies, checksums (should be offloaded).

S3 service-side: watch per-request latencies and any throttling (respect retry/backoff).


8) Concrete starting recipes

A) On-prem → S3 (high RTT, say 120–200 ms)

Enable Transfer Acceleration (test; often 1.2–3× boost).

Multipart: 64–128 MiB parts, concurrency 16–48.

TCP: BBR, wmem_max=256MB, tcp_wmem as above, sch_fq qdisc.

NIC: XPS set, rings 4096, coalescing 64–128 µs.

Tune until throughput stops increasing when raising concurrency.


B) EC2 (same region as bucket)

Use S3 VPC Endpoint, ENA, placement group if pushing >25–50 Gbps from multiple instances.

MTU 9001 in VPC path; 64 MiB parts, concurrency 8–16 is often enough (low RTT).

BBR vs CUBIC: try both; intra-region RTT is tiny, CUBIC may perform just as well.


9) Common “gotchas” that kill throughput

Too small part size (e.g., 5 MiB) with low concurrency → TLS/HTTP overhead dominates.

Re-establishing TLS/HTTP connection per part (keep-alive disabled).

CPU pegged on TLS (no AES-NI/ChaCha), or upload threads not pinned → low effective Gbps.

Single stream only on high-RTT links: cwnd/BDP underfilled; use multipart parallelism.

Jumbo MTU enabled on one side of the path only → black-holes; verify PMTU.

Overzealous netfilter rules adding per-packet cost on egress; keep hot paths short.



---

Quick checklist (paste-n-go)

Linux host

# Pacing & CC
tc qdisc replace dev eth0 root fq
sysctl -w net.ipv4.tcp_congestion_control=bbr
sysctl -w net.core.wmem_max=268435456
sysctl -w net.ipv4.tcp_wmem="4096 87380 268435456"

# NIC
ethtool -K eth0 tso on gso on tx on
ethtool -G eth0 tx 4096
ethtool -C eth0 tx-usecs 64
ip link set dev eth0 txqueuelen 3000
# (optional, DC/VPC only)
# ip link set dev eth0 mtu 9000

# XPS (example masks)
echo 0-7   > /sys/class/net/eth0/queues/tx-0/xps_cpus
echo 8-15  > /sys/class/net/eth0/queues/tx-1/xps_cpus

SDK (example boto3)

from boto3.s3.transfer import TransferConfig
cfg = TransferConfig(
    multipart_threshold = 8*1024**2,
    multipart_chunksize = 64*1024**2,  # try 64–128 MiB
    max_concurrency     = 32,          # try 16–48 on high RTT
    use_threads=True
)
# s3.upload_file(F, bucket, key, Config=cfg)


---

If you tell me (a) where the uploader runs (on-prem city ↔ S3 region, or EC2 region), (b) link speed/RTT, (c) CPU model/cores, I’ll return a part-size & concurrency plan (numbers) + exact SDK settings to saturate your pipe without spiking CPU.

